{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q omegaconf watermark\n",
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from omegaconf import OmegaConf\n",
    "import wandb\n",
    "from wandb.keras import WandbMetricsLogger, WandbModelCheckpoint, WandbEvalCallback\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "np.set_printoptions(precision=4)\n",
    "%watermark --iversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "api_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
    "wandb.login(key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = OmegaConf.create(\n",
    "    dict(\n",
    "        data_path=\"/kaggle/input/utkface-cropped/UTKFace/\",\n",
    "        img_size=(200, 200),\n",
    "        target_size=(200, 200),\n",
    "        wandb_project=\"UTKFace-Age-Regression\",\n",
    "        wandb_group=\"Baseline\",\n",
    "        models_dir=\"models\",\n",
    "        use_sample_weight=False,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cfg = OmegaConf.create(\n",
    "    dict(\n",
    "        architecture=\"Simple CNN\",\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        lr_schedule=\"ExponentialDecay\",\n",
    "        initial_learning_rate=1e-3,\n",
    "        decay_steps=100000,\n",
    "        decay_rate=0.96,\n",
    "        loss=\"mean_absolute_error\",\n",
    "        optimizer=\"Adam\",\n",
    "        early_stopping_patience=5,\n",
    "        early_stopping_monitor=\"val_mae\",\n",
    "        early_stopping_mode=\"min\",\n",
    "        random_translation=0.1,\n",
    "        random_rotation=0.15,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the training and testing dataset using the recommended tf.data API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.list_files(cfg.data_path + \"*\")\n",
    "\n",
    "\n",
    "def process_path(file_path):\n",
    "    # read the age from the filename\n",
    "    filename = tf.strings.split(file_path, os.sep)[-1]\n",
    "    label = tf.strings.split(filename, \"_\")[0]\n",
    "    label = tf.strings.to_number(label, out_type=tf.dtypes.int32)\n",
    "\n",
    "    # read and decode the image\n",
    "    raw = tf.io.read_file(file_path)\n",
    "    image = tf.image.decode_jpeg(raw, channels=3)\n",
    "    print(\"Initial shape: \", image.shape)\n",
    "    image.set_shape([200, 200, 3])\n",
    "    print(\"Final shape: \", image.shape)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "labeled_dataset = dataset.map(process_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, label in labeled_dataset.take(1):\n",
    "    print(\"Image shape: \", img.numpy().shape)\n",
    "    print(\"Label: \", label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, test_ds = tf.keras.utils.split_dataset(\n",
    "    labeled_dataset, left_size=0.8, shuffle=True\n",
    ")\n",
    "len(train_ds), len(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(config):\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            Input(shape=(64, 64, 3)),\n",
    "            Conv2D(128, kernel_size=(3, 3), activation=\"relu\"),\n",
    "            Conv2D(128, kernel_size=(3, 3), activation=\"relu\"),\n",
    "            MaxPooling2D(),\n",
    "            Dropout(0.25),\n",
    "            Conv2D(128, kernel_size=(3, 3), activation=\"relu\"),\n",
    "            Conv2D(128, kernel_size=(3, 3), activation=\"relu\"),\n",
    "            MaxPooling2D(),\n",
    "            Dropout(0.25),\n",
    "            Conv2D(128, kernel_size=(3, 3), activation=\"relu\"),\n",
    "            Conv2D(128, kernel_size=(3, 3), activation=\"relu\"),\n",
    "            MaxPooling2D(),\n",
    "            Flatten(),\n",
    "            Dropout(0.25),\n",
    "            Dense(\n",
    "                1, activation=\"relu\"\n",
    "            ),  # we only need positive integers as output, therefore relu activation\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=config.initial_learning_rate,\n",
    "        decay_steps=config.decay_steps,\n",
    "        decay_rate=config.decay_rate,\n",
    "    )\n",
    "    metrics = [\"mae\"]\n",
    "    weighted_metrics = (\n",
    "        [keras.metrics.MeanAbsoluteError(name=\"mae_weighted\")]\n",
    "        if cfg.use_sample_weight\n",
    "        else None\n",
    "    )\n",
    "    model.compile(\n",
    "        loss=config.loss,\n",
    "        optimizer=Adam(learning_rate=lr_schedule),\n",
    "        metrics=metrics,\n",
    "        weighted_metrics=weighted_metrics,\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=model_cfg.early_stopping_monitor,\n",
    "    verbose=1,\n",
    "    patience=model_cfg.early_stopping_patience,\n",
    "    mode=model_cfg.early_stopping_mode,\n",
    "    restore_best_weights=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement model prediction visualization callback\n",
    "class WandbClfEvalCallback(WandbEvalCallback):\n",
    "    \"\"\"Classification Evaluation Callback that logs predictions to Weights and biases.\n",
    "\n",
    "    This Callback runs after each epoch and logs a single batch of predictions\"\"\"\n",
    "\n",
    "    def __init__(self, validation_data, data_table_columns, pred_table_columns):\n",
    "        super().__init__(data_table_columns, pred_table_columns)\n",
    "\n",
    "        self.data = validation_data\n",
    "\n",
    "    def add_ground_truth(self, logs=None):\n",
    "        # TODO: sample weight support\n",
    "        for images, labels in self.data.take(1).as_numpy_iterator():\n",
    "            for idx, (img, label) in enumerate(zip(images, labels)):\n",
    "                self.data_table.add_data(idx, wandb.Image(img), label)\n",
    "\n",
    "    def add_model_predictions(self, epoch, logs=None):\n",
    "        preds = self.model.predict(self.data, verbose=0)\n",
    "\n",
    "        table_idxs = self.data_table_ref.get_index()\n",
    "\n",
    "        for idx in table_idxs:\n",
    "            pred = preds[idx][0]\n",
    "            self.pred_table.add_data(\n",
    "                epoch,\n",
    "                self.data_table_ref.data[idx][0],\n",
    "                self.data_table_ref.data[idx][1],\n",
    "                self.data_table_ref.data[idx][2],\n",
    "                pred,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_and_rescale = tf.keras.Sequential([Resizing(64, 64), Rescaling(1.0 / 255)])\n",
    "\n",
    "data_augmentation = tf.keras.Sequential(\n",
    "    [\n",
    "        RandomRotation(model_cfg.random_rotation),\n",
    "        RandomTranslation(\n",
    "            width_factor=model_cfg.random_translation,\n",
    "            height_factor=model_cfg.random_translation,\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "\n",
    "def prepare(ds, shuffle=False, augment=False):\n",
    "    # Resize and rescale all datasets.\n",
    "    ds = ds.map(lambda x, y: (resize_and_rescale(x), y), num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(1000)\n",
    "\n",
    "    # Batch all datasets.\n",
    "    ds = ds.batch(model_cfg.batch_size)\n",
    "\n",
    "    # Use data augmentation only on the training set.\n",
    "    if augment:\n",
    "        ds = ds.map(\n",
    "            lambda x, y: (data_augmentation(x, training=True), y),\n",
    "            num_parallel_calls=AUTOTUNE,\n",
    "        )\n",
    "\n",
    "    # Use buffered prefetching on all datasets.\n",
    "    return ds.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = prepare(train_ds, shuffle=True, augment=True)\n",
    "test_ds = prepare(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(\n",
    "    project=cfg.wandb_project,\n",
    "    group=cfg.wandb_group,\n",
    "    config=OmegaConf.to_object(model_cfg),\n",
    "    tags=[\"Baseline\", \"Image Augmentation\", \"tf.data API\"],\n",
    "    notes=\"Baseline CNN Model with image augmentation (tf.data API)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(model_cfg)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    early_stopping,\n",
    "    WandbMetricsLogger(),\n",
    "    WandbModelCheckpoint(cfg.models_dir, monitor=model_cfg.early_stopping_monitor),\n",
    "    WandbClfEvalCallback(\n",
    "        validation_data=test_ds,\n",
    "        data_table_columns=[\"idx\", \"image\", \"label\"],\n",
    "        pred_table_columns=[\"epoch\", \"idx\", \"image\", \"label\", \"pred\"],\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%wandb\n",
    "model.fit(\n",
    "    train_ds,\n",
    "    epochs=model_cfg.epochs,\n",
    "    validation_data=test_ds,\n",
    "    callbacks=callbacks,\n",
    "    use_multiprocessing=True,\n",
    "    workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
